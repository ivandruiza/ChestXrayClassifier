{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12133254,"sourceType":"datasetVersion","datasetId":7610161}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final Database\nAfter a manual inspection of the 1000 samples this ended up with 450 normal x ray vs. 550 not normal x ray. The big number of “mislabeled” data is mostly explained by the fact that this database was intended for a different classifier. It has tags on some specific pathologies, and the absence of those diseases is considered “No findings” it does not mean it is normal. Most of the not normal x ray in this sample are because they have catheters, some of them come from people with tracheal tubes and others are clearly mislabeled. But with this curated data I had the fortune to create two separate groups that are almost balanced. <br>\nI have here to confess I wanted more x rays, but this manual inspection is time consuming, so for now I will stay with this 1000 sample. I discarted 2 img because they were lateral proyections, and i manually fixed 2 inverted colored img.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T16:50:03.189318Z","iopub.execute_input":"2025-06-11T16:50:03.189549Z","iopub.status.idle":"2025-06-11T16:50:03.993377Z","shell.execute_reply.started":"2025-06-11T16:50:03.189527Z","shell.execute_reply":"2025-06-11T16:50:03.992529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Loading dataframe\ndf = pd.read_csv('/kaggle/input/random-sample-of-adults-nih-normal-chest-x-rays/df_sample.csv')\ndf.drop(columns=['Finding Labels'], inplace=True)\n\n#variables  for functions\nnormal_x = set(os.listdir('/kaggle/input/random-sample-of-adults-nih-normal-chest-x-rays/sampled_images_1000/sampled_images_1000'))\nnot_x = set(os.listdir('/kaggle/input/random-sample-of-adults-nih-normal-chest-x-rays/sampled_images_1000/sampled_images_1000/eliminated'))\nfolders = [\n    '/kaggle/input/random-sample-of-adults-nih-normal-chest-x-rays/sampled_images_1000/sampled_images_1000',\n    '/kaggle/input/random-sample-of-adults-nih-normal-chest-x-rays/sampled_images_1000/sampled_images_1000/eliminated'\n]\n\n# Funcitions to get labels and paths\ndef get_label(image_name):\n    if image_name in normal_x:\n        return 0\n    elif image_name in not_x:\n        return 1\n    else:\n        return pd.NaT\n        \ndef find_image_path(filename):\n    for folder in folders:\n        full_path = os.path.join(folder, filename)\n        if os.path.exists(full_path):\n            return full_path\n    return None  # si no se encontró\n\n# Use functions to get labels and paths\ndf['label'] = df['Image Index'].apply(get_label)\ndf['label'].unique()\ndf['image_path'] = df['Image Index'].apply(find_image_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T16:52:54.798235Z","iopub.execute_input":"2025-06-11T16:52:54.798484Z","iopub.status.idle":"2025-06-11T16:52:55.347344Z","shell.execute_reply.started":"2025-06-11T16:52:54.798467Z","shell.execute_reply":"2025-06-11T16:52:55.346737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna()\ndf.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T16:53:32.331983Z","iopub.execute_input":"2025-06-11T16:53:32.332423Z","iopub.status.idle":"2025-06-11T16:53:32.342776Z","shell.execute_reply.started":"2025-06-11T16:53:32.332403Z","shell.execute_reply":"2025-06-11T16:53:32.342137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling\nNow we are on track to create our model. Searching on medical database Kufle et al. (1) hay a good performance when using Densenet 121 pretrain (2), which for us comes in handy because we ended up with a small to medium dataset so a pretrain network and transfer learning can help us now.\n\nAlso pytorch has a good documentation on how to implement this network. (3, 4)\n\n“Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.” (2)\n\n\n1.\tKufel J, Bielówka M, Rojek M, Mitręga A, Lewandowski P, Cebula M, Krawczyk D, Bielówka M, Kondoł D, Bargieł-Łączek K, Paszkiewicz I, Czogalik Ł, Kaczyńska D, Wocław A, Gruszczyńska K, Nawrat Z. Multi-Label Classification of Chest X-ray Abnormalities Using Transfer Learning Techniques. J Pers Med. 2023 Sep 22;13(10):1426. doi: 10.3390/jpm13101426. PMID: 37888037; PMCID: PMC10607847.\n2.\tHuang, G., Liu, Z., van der Maaten, L., & Weinberger, K. Q. (2017). Densely Connected Convolutional Networks. Computer Vision and Pattern Recognition, 2261–2269. https://doi.org/10.1109/CVPR.2017.243\n3.\thttps://docs.pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport copy\n\nfrom torchvision import transforms\nfrom torchvision.io import read_image\nfrom torch.utils.data import Dataset\nfrom torchvision import models\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:12.013591Z","iopub.execute_input":"2025-06-11T15:16:12.013940Z","iopub.status.idle":"2025-06-11T15:16:15.090312Z","shell.execute_reply.started":"2025-06-11T15:16:12.013912Z","shell.execute_reply":"2025-06-11T15:16:15.089708Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: class creation and splitting data\nI am not using any data augmentation but chest x-rays are going to need a transformation for they to look as ImageNet data because the network is expecting to have this kind of input.\n","metadata":{}},{"cell_type":"code","source":"class ChestXrayDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        path = self.df.loc[idx, 'image_path']\n        label = self.df.loc[idx, 'label']\n        #load image\n        image = Image.open(path).convert(\"RGB\")\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        return image, torch.tensor(label, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:15.091020Z","iopub.execute_input":"2025-06-11T15:16:15.091413Z","iopub.status.idle":"2025-06-11T15:16:15.096507Z","shell.execute_reply.started":"2025-06-11T15:16:15.091392Z","shell.execute_reply":"2025-06-11T15:16:15.095786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (224x224px - Normalization -> ImageNet)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Imagenet mean\n                         std=[0.229, 0.224, 0.225])    # Imagenet std\n])\n\n\n# 1. Split test-train\ndf_train, df_val = train_test_split(df, stratify=df[\"label\"], test_size=0.2, random_state=1024)\n\n# 2. Datasets\ntrain_dataset = ChestXrayDataset(df_train, transform=transform) #normal + abnormal\nval_dataset = ChestXrayDataset(df_val, transform=transform)\ntest_dataset = ChestXrayDataset(df.copy(), transform=transform)\n\n# 3. DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:15.098890Z","iopub.execute_input":"2025-06-11T15:16:15.099279Z","iopub.status.idle":"2025-06-11T15:16:15.121800Z","shell.execute_reply.started":"2025-06-11T15:16:15.099263Z","shell.execute_reply":"2025-06-11T15:16:15.121256Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Print images\nclasses = ['Normal', 'Not-normal']\n\ndef show_image_grid(images, labels, unnormalize=False):\n    fig, axes = plt.subplots(2, 8, figsize=(16, 5))\n    for i in range(16):\n        img = images[i]\n        label = labels[i]\n\n        if unnormalize:\n            img = img / 2 + 0.5  # unnormalize\n\n        npimg = img.numpy()\n        npimg = np.transpose(npimg, (1, 2, 0))  # CHW to HWC\n\n        ax = axes[i // 8, i % 8]\n        ax.imshow(npimg)\n        ax.set_title(classes[int(label.item())])\n        ax.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n# Get a batch\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n\n# Show in 8x2 grid\nshow_image_grid(images, labels, unnormalize=True)  # set to True if needed\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:15.122443Z","iopub.execute_input":"2025-06-11T15:16:15.122626Z","iopub.status.idle":"2025-06-11T15:16:16.494924Z","shell.execute_reply.started":"2025-06-11T15:16:15.122611Z","shell.execute_reply":"2025-06-11T15:16:16.494116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2: Initial configuration of model","metadata":{}},{"cell_type":"code","source":"# https://docs.pytorch.org/vision/main/models/generated/torchvision.models.densenet121.html\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# DenseNet-121\nmodel = models.densenet121(weights = 'DenseNet121_Weights.IMAGENET1K_V1')\n\n# (classifier) (0 - 1)\nnum_features = model.classifier.in_features\nmodel.classifier = nn.Sequential(\n    nn.Linear(num_features, 1),      # 1 output\n)\n\nmodel = model.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nnum_epochs = 10\n\ncheckpoint_path = '/kaggle/working/model/'\nos.makedirs(checkpoint_path, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:16.495822Z","iopub.execute_input":"2025-06-11T15:16:16.496066Z","iopub.status.idle":"2025-06-11T15:16:16.864223Z","shell.execute_reply.started":"2025-06-11T15:16:16.496047Z","shell.execute_reply":"2025-06-11T15:16:16.863618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, checkpoint_path, val_loader=None):\n    model.train()  # Mode\n    running_loss = 0.0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device).float().unsqueeze(1)  # shape [B, 1]\n\n        optimizer.zero_grad()          # 1. Clean\n        outputs = model(images)        # 2. Forward\n        loss = criterion(outputs, labels)  # 3. Loss\n        loss.backward()                # 4. Backpropagation\n        optimizer.step()               # 5. Weights\n\n        running_loss += loss.item() * images.size(0)  # Total loss\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n\n    # save\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': epoch_loss,\n    }, f\"{checkpoint_path}/checkpoint_epoch_{epoch}.pt\")\n    \n    return epoch_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:16.864974Z","iopub.execute_input":"2025-06-11T15:16:16.865178Z","iopub.status.idle":"2025-06-11T15:16:16.870565Z","shell.execute_reply.started":"2025-06-11T15:16:16.865161Z","shell.execute_reply":"2025-06-11T15:16:16.869805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3: def Evaluate ","metadata":{}},{"cell_type":"code","source":"def evaluate(model, val_loader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device).float().unsqueeze(1)\n\n            outputs = model(images)\n            probs = torch.sigmoid(outputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item() * images.size(0)\n\n            preds = (outputs >= 0.5).int()\n            all_preds.append(preds.cpu())\n            all_labels.append(labels.cpu().int())\n\n    # all batches\n    all_preds = torch.cat(all_preds)\n    all_labels = torch.cat(all_labels)\n\n    # FP & FN\n    FP = ((all_preds == 1) & (all_labels == 0)).sum().item()\n    FN = ((all_preds == 0) & (all_labels == 1)).sum().item()\n\n    avg_loss = val_loss / len(val_loader.dataset)\n    \n    return avg_loss, FP, FN, all_preds, all_labels\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:16.871355Z","iopub.execute_input":"2025-06-11T15:16:16.871675Z","iopub.status.idle":"2025-06-11T15:16:16.889105Z","shell.execute_reply.started":"2025-06-11T15:16:16.871655Z","shell.execute_reply":"2025-06-11T15:16:16.888398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4: Training","metadata":{}},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, checkpoint_path)\n    val_loss, FP, FN, val_preds, val_labels = evaluate(model, val_loader, criterion, device)\n\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Train Loss: {train_loss:.4f}\")\n    print(f\"Validation Loss:    {val_loss:.4f}\")\n    print(f\"FP:      {FP}\")\n    print(f\"FN:      {FN}\")\n    # print(val_preds.numpy().flatten())\n\n# images in validation set: print(val_dataset.df['image_path'].tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:16:16.889981Z","iopub.execute_input":"2025-06-11T15:16:16.890269Z","iopub.status.idle":"2025-06-11T15:20:32.696388Z","shell.execute_reply.started":"2025-06-11T15:16:16.890241Z","shell.execute_reply":"2025-06-11T15:20:32.695512Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter tuning\n\nAlthough those are decent results it seems clear this is a hard task. Loss functions tend to go down but the number of mislabeled tend does not go down the same way. Also is evident that after a while the network is overfitting. <br>\nHere we try various optimizers and at the same time play with the learning rate to see what is happening. Also, in this problem I believe false negatives are more problematic than false positive. Imagine you are at the ER with pneumonia and the CAD tells your doctor you are okey. I prefer to be okey and the CAD to be a little dramatic, so in the Loss function we are penalizing those FN harder.","metadata":{}},{"cell_type":"code","source":"pos_weight = torch.tensor([2.0]).to(device)\ncriterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n\ndef train_model_with_optimizer(optimizer_name, model, train_loader, val_loader, criterion, device, epochs=10):\n    model_copy = copy.deepcopy(model)\n    \n    if optimizer_name == 'adam':\n        optimizer = torch.optim.Adam(model_copy.parameters(), lr=1e-5)\n    elif optimizer_name == 'adamw':\n        optimizer = torch.optim.AdamW(model_copy.parameters(), lr=1e-4, weight_decay=1e-2)\n    elif optimizer_name == 'sgd':\n        optimizer = torch.optim.SGD(model_copy.parameters(), lr=1e-4, momentum=0.9)\n    else:\n        raise ValueError(f\"Optimizer {optimizer_name} no soportado\")\n    \n    checkpoint_path = f'/kaggle/working/othermodels/{optimizer_name}'\n    os.makedirs(checkpoint_path, exist_ok=True)\n\n    for epoch in range(epochs):\n        train_loss = train_one_epoch(model_copy, train_loader, criterion, optimizer, device, epoch+1, checkpoint_path, val_loader)\n        val_loss, FP, FN, val_preds, y_true = evaluate(model_copy, val_loader, criterion, device)\n        \n        print(f\"Epoch {epoch+1}/{epochs}\")\n        print(f\"🔹 Train Loss: {train_loss:.4f}\")\n        print(f\"🔹 Val Loss:    {val_loss:.4f}\")\n        print(f\"🔸 FP:      {FP}\")\n        print(f\"🔸 FN:      {FN}\")\n        # preditions:  print(val_preds.numpy().flatten())\n\n    return model_copy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:20:32.697338Z","iopub.execute_input":"2025-06-11T15:20:32.697933Z","iopub.status.idle":"2025-06-11T15:20:32.705072Z","shell.execute_reply.started":"2025-06-11T15:20:32.697905Z","shell.execute_reply":"2025-06-11T15:20:32.704347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_roc(y_true, y_scores, label=None):\n    fpr, tpr, _ = roc_curve(y_true, y_scores)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.3f})')\n    return roc_auc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:20:32.705721Z","iopub.execute_input":"2025-06-11T15:20:32.705936Z","iopub.status.idle":"2025-06-11T15:20:32.719844Z","shell.execute_reply.started":"2025-06-11T15:20:32.705920Z","shell.execute_reply":"2025-06-11T15:20:32.719109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizers = ['adam', 'adamw', 'sgd']\nmodels = {}\nresults = {}\n\nfor opt_name in optimizers:\n    print(f\"\\nOptimizer: {opt_name}\")\n    trained_model = train_model_with_optimizer(opt_name, model, train_loader, val_loader, criterion, device, epochs=10)\n    models[opt_name] = trained_model\n\n    _, _, _, y_scores, y_true = evaluate(trained_model, val_loader, criterion, device)\n    results[opt_name] = (y_true, y_scores)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:20:32.720523Z","iopub.execute_input":"2025-06-11T15:20:32.720689Z","iopub.status.idle":"2025-06-11T15:33:26.236943Z","shell.execute_reply.started":"2025-06-11T15:20:32.720676Z","shell.execute_reply":"2025-06-11T15:33:26.236286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  ROC curves\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\n\nplt.figure(figsize=(8,6))\nfor opt_name, (y_true, y_scores) in results.items():\n    plot_roc(y_true, y_scores, label=opt_name)\nplt.plot([0,1], [0,1], 'k--')  # línea diagonal para referencia\nplt.xlabel(\"Falso Positivo\")\nplt.ylabel(\"Verdadero Positivo\")\nplt.title(\"Curvas ROC - Comparación de optimizadores\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:33:26.237740Z","iopub.execute_input":"2025-06-11T15:33:26.237946Z","iopub.status.idle":"2025-06-11T15:33:26.418796Z","shell.execute_reply.started":"2025-06-11T15:33:26.237931Z","shell.execute_reply":"2025-06-11T15:33:26.418109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def print_confusion_matrix(y_true, y_scores, threshold):\n    y_pred = (y_scores >= threshold).int().cpu().numpy()\n    cm = confusion_matrix(y_true, y_pred)\n    print(\"Matrix (threshold = {:.2f}):\".format(threshold))\n    print(cm)\n\nfor opt_name, (y_true, y_scores) in results.items():\n    print(f\"\\nOptimizador: {opt_name}\")\n    print_confusion_matrix(y_true, y_scores, threshold=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:33:26.419683Z","iopub.execute_input":"2025-06-11T15:33:26.419986Z","iopub.status.idle":"2025-06-11T15:33:26.430271Z","shell.execute_reply.started":"2025-06-11T15:33:26.419964Z","shell.execute_reply":"2025-06-11T15:33:26.429661Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results & Discution\nWe are now going to use our best model and check mistakes by hand.\n\nWhen comparing the ROC curves, we see they are similar, but taking into account the balance between FN and FP I am going to take the one that I consider the most suited to this problem. Lets see FN.","metadata":{},"attachments":{}},{"cell_type":"code","source":"# Load \ndf_test = df[['image_path', 'label']].copy()\nmodel.to(device)\n\n# Choose checkpoint\ncheckpoint_path = '/kaggle/working/othermodels/adamw/checkpoint_epoch_8.pt'\n\n# Load checkpoint & eval\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\npredicted_classes = []\nprediction_scores = []\n\nfor _, row in df_test.iterrows():\n    img_path = row['image_path']\n    \n    # Load\n    pil_img = Image.open(img_path).convert(\"RGB\")\n    input_tensor = transform(pil_img).unsqueeze(0).to(device)\n    input_tensor.requires_grad_()\n\n    # Forward\n    output = model(input_tensor)\n    prediction = torch.sigmoid(output).item()\n    predicted_class = 1 if prediction > 0.5 else 0\n\n    # Save\n    predicted_classes.append(predicted_class)\n    prediction_scores.append(prediction)\n\n# DataFrame\ndf_test['predicted_class'] = predicted_classes\ndf_test['score'] = prediction_scores\n\n\"\"\"\nCheck DF\n\nprint(df_test.head())\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T16:29:43.446108Z","iopub.execute_input":"2025-06-11T16:29:43.446414Z","iopub.status.idle":"2025-06-11T16:30:23.007847Z","shell.execute_reply.started":"2025-06-11T16:29:43.446396Z","shell.execute_reply":"2025-06-11T16:30:23.007152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_missed = df_test[df_test['label'] != df_test['predicted_class']]\ndf_missed.to_csv('/kaggle/working/df_missed.csv', index=False)\ndf_missed.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T16:30:37.385035Z","iopub.execute_input":"2025-06-11T16:30:37.385925Z","iopub.status.idle":"2025-06-11T16:30:37.396434Z","shell.execute_reply.started":"2025-06-11T16:30:37.385900Z","shell.execute_reply":"2025-06-11T16:30:37.395802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_FN = df_missed[(df_missed['label'] == 1) & (df_missed['predicted_class'] == 0)]\ncolumns = 5\ntotal = len(df_FN)\nrows = (total + columns - 1) // columns\n\nplt.figure(figsize=(columns * 3, rows * 3))\n\nfor i, image_path in enumerate(df_FN['image_path']):\n    pil_img = Image.open(image_path).convert(\"RGB\")\n\n    plt.subplot(rows, columns, i + 1)\n    plt.imshow(pil_img)\n    plt.axis('off')\n    label = int(df_FN.iloc[i]['label']) \n    pred = int(df_FN.iloc[i]['predicted_class']) \n    plt.title(f\"True: {label} | Pred: {pred}\", fontsize=10)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T16:37:29.775182Z","iopub.execute_input":"2025-06-11T16:37:29.775832Z","iopub.status.idle":"2025-06-11T16:37:38.856482Z","shell.execute_reply.started":"2025-06-11T16:37:29.775808Z","shell.execute_reply":"2025-06-11T16:37:38.855253Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion and final thoughts\nThis is not the best classifier. It has a margin for improvement because it got wrong 10% of cases, even more if you consider df_test includes images from training, and I can see some big mistakes here. This is a difficult problem as sometimes normal from not normal is only a subtle difference. For future work I believe we need a bigger dataset, we need to test other types of CNN or create a custom one, we need to confirm my tags with colleagues, we can play a little bit more with the learning rate and the punishment for those FN. <br>\nIn medical images there is so much to work with labels.\nIt seems that this network is having trouble detecting those invasive catheters and tubes. Can this be because of the low resolution? Do we need special convolutional filter to capture this? Can we play with contrast to make catheters more visible?<br>\nAlso we did not play and check with details score given by my network, a better treshold can help with FN.\n\n# How does it classify that way?\nTo finish this work I am going to see hoy this is deciding using GradCAM. (1)\n\n1.\thttps://datascientest.com/es/que-es-el-metodo-grad-cam","metadata":{}},{"cell_type":"code","source":"!pip install torchcam --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:34:10.785093Z","iopub.execute_input":"2025-06-11T15:34:10.785624Z","iopub.status.idle":"2025-06-11T15:35:25.880042Z","shell.execute_reply.started":"2025-06-11T15:34:10.785599Z","shell.execute_reply":"2025-06-11T15:35:25.879213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchcam.methods import GradCAM\nfrom torchcam.utils import overlay_mask\nfrom torchvision.transforms.functional import to_pil_image\nfrom torchcam.utils import overlay_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:35:25.884709Z","iopub.execute_input":"2025-06-11T15:35:25.885041Z","iopub.status.idle":"2025-06-11T15:35:25.898387Z","shell.execute_reply.started":"2025-06-11T15:35:25.885014Z","shell.execute_reply":"2025-06-11T15:35:25.897681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. IMG\nimage_path = \"/kaggle/input/random-sample-of-adults-nih-normal-chest-x-rays/sampled_images_1000/sampled_images_1000/eliminated/00000193_003.png\"\npil_img = Image.open(image_path).convert(\"RGB\")\nplt.imshow(pil_img, cmap='gray')\n\n# 2. transform\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ninput_tensor = transform(pil_img).unsqueeze(0).to(device)\ninput_tensor.requires_grad_()  # ¡Importante para GradCAM!\n\n# 3. Eval\nmodel.eval()\n\n# 4. CAM\ncam_extractor = GradCAM(model, target_layer=\"features.denseblock4\")\n\n# 5. Forward\noutput = model(input_tensor)\n\n# 6. Predition\nprediction = torch.sigmoid(output).item()\npredicted_class = 1 if prediction > 0.5 else 0\n\nprint(f\"Predicción: {'NOT - Normal' if predicted_class == 1 else 'Normal'} (score: {prediction:.4f})\")\n\n# 7. Activación CAM\nactivation_map = cam_extractor(0, output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:35:25.899205Z","iopub.execute_input":"2025-06-11T15:35:25.899397Z","iopub.status.idle":"2025-06-11T15:35:26.488021Z","shell.execute_reply.started":"2025-06-11T15:35:25.899381Z","shell.execute_reply":"2025-06-11T15:35:26.487256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show img + activation\npil_img = Image.open(image_path).convert(\"RGB\")\ncam = to_pil_image(activation_map[0].cpu(), mode='F').resize(pil_img.size)\nresult = overlay_mask(pil_img, cam, alpha=0.6)\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(1, 2, 1)\nplt.imshow(pil_img, cmap='gray')\nplt.title(\"Original\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(result)\nplt.title(\"Grad-CAM Overlay\")\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this example is evident to me although the image is well classified tracheal tube is missed so this is another problem to work on.\n\n**This code would have been impossible for me without the help of ChatGPT.**\n","metadata":{}},{"cell_type":"code","source":"# rm -rf /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:35:27.059573Z","iopub.execute_input":"2025-06-11T15:35:27.059838Z","iopub.status.idle":"2025-06-11T15:35:27.063546Z","shell.execute_reply.started":"2025-06-11T15:35:27.059818Z","shell.execute_reply":"2025-06-11T15:35:27.062790Z"}},"outputs":[],"execution_count":null}]}